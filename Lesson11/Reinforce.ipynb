{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"reinforce.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"VbCJU6pY27HZ","colab_type":"text"},"source":["https://www.youtube.com/watch?v=ozldkmdZ3lU\n","# REINFORCE\n","\n","Пусть $X$ это какая-то случайная величина с известным распределением $p_\\theta(X)$, где $\\theta$ — это параметры модели. Определим функцию награды $J(\\theta)$ как\n","\n","$$ J(\\theta) = E[f(X)] = \\int_x f(x) p(x) \\; dx $$\n","\n","для произвольной функции $f$, не зависящей от $\\theta$.\n","\n","*Policy gradient* (дословно, градиент стратегии) — это, интуитивно, то направление, куда нужно двигать параметры модели, чтобы функция награды увеличивалась. Алгоритм REINFORCE (почему его всегда пишут капсом автор не знает) на каждом шаге просто определяет policy gradient $\\nabla_\\theta J(\\theta)$ и изменяет параметры в его сторону с каким-то learning rate-ом.\n","\n","**Зачем это надо**, если есть обычный градиентный спуск? Через policy gradient и reinforcement learning вообще можно оптимизировать более общий класс функций — хоть дискретные (например, можно BLEU для перевода напрямую максимизировать) и даже невычислимые (какие-нибудь субъективные оценки асессоров).\n","\n","В частности, в таком ключе можно описать игры (для простоты, однопользовательские): есть какие-то награды за совершение каких-то действий (для шахмат: +1 за победу, 0 за ничью, -1 за поражение; для тетриса: +0.1 за «выживание» ещё одну секунду, 1 за удаление слоя, 0 за проигрыш) и нам нужно подобрать такие параметры модели, чтобы максимизировать ожидаемую сумму наград за действия, которые мы совершили, то есть в точности $J(\\theta)$.\n","\n","Теперь немного математики: как нам найти этот $\\nabla_\\theta J(\\theta)$? Оказывается, мы можем выразить его ожидание, а тогда приблизительный градиент можно будет находить сэмплированием и усреднением градиентов — так же, как мы обычно обучаем нейросети на батчах.\n","\n","$$ J(\\theta) = E[f(X)] = \\int_x f(x) p(x) \\; dx = \\int_x f(x) p(x) \\nabla_\\theta \\log p(x) \\; dx = E[f(x) \\nabla_\\theta \\log p(x)] $$\n","\n","Переход между 2 и 3 верен, потому что $\\nabla \\log p(x) = \\frac{\\nabla p(x)}{p(x)}$ (просто подставьте и $p(x)$ сократится). Это называют log-derivative trick.\n","\n","Мы научились получить приблизительный градиент через сэмплирование. Давайте теперь что-нибудь обучим."]},{"cell_type":"markdown","metadata":{"id":"49kATdp-27Hc","colab_type":"text"},"source":["## `CartPole-v0`"]},{"cell_type":"markdown","metadata":{"id":"1rY14MXK27Hd","colab_type":"text"},"source":["Про задачу можно почитать тут: https://gym.openai.com/envs/CartPole-v0/. Tl;dr: есть вертикально стоящая палка на подвижной платформе; нужно двигать платформу так, чтобы палка не упала.\n","\n","<img width='500px' src='https://preview.redd.it/sqjzj2cgnpt21.gif?overlay-align=bottom,left&overlay-pad=8,16&crop=1200:628.272251309,smart&overlay-height=0.10&overlay=%2Fv9vyirk6hl221.png%3Fs%3Db466421949eb723078743745ce6421609d7a9c66&width=1200&height=628.272251309&s=ba84ac5a9c14946456808c15f2754cb7369b8de9'>\n","\n","OpenAI в 2016-м году выпустили `gym` — библиотечку для абстрагирования RL-ных сред от алгоритма. Есть абстрактная *среда* (`env`), в ней есть какие-то *состояния* (`state`), из каждого состояния есть какой-то фиксированный набо *действий* (`action`), ведущих (возможно, с какими-то вероятностями) в другие состояния, и за разные действия в разных состояниях дается какая-то *награда* (`reward`). Как конкретно устроена игра, нам думать не нужно."]},{"cell_type":"code","metadata":{"id":"YbExof5D27Hf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"e42124c1-2832-42cc-f533-caa765193f72","executionInfo":{"status":"ok","timestamp":1576339664068,"user_tz":-180,"elapsed":3591,"user":{"displayName":"Sergey Slotin","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mASW7k19zkwsKgs2yTuF86CKOmEyG-vloH_x2gz=s64","userId":"13456807618518804390"}}},"source":["import os\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n","    !bash ../xvfb start\n","    %env DISPLAY=:1"],"execution_count":1,"outputs":[{"output_type":"stream","text":["bash: ../xvfb: No such file or directory\n","env: DISPLAY=:1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a32D48N427Hl","colab_type":"code","colab":{}},"source":["import gym\n","import numpy as np, pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","env = gym.make(\"CartPole-v0\").env\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape\n","\n","#plt.imshow(env.render(\"rgb_array\"))\n","env.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4tM1-JRK0Nz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"22632c9a-fd15-4ade-d4fa-0cb9ed2d6832","executionInfo":{"status":"ok","timestamp":1576339780896,"user_tz":-180,"elapsed":673,"user":{"displayName":"Sergey Slotin","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mASW7k19zkwsKgs2yTuF86CKOmEyG-vloH_x2gz=s64","userId":"13456807618518804390"}}},"source":["state_dim"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4,)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"MHbZLha-27Hr","colab_type":"text"},"source":["## Сеть\n","\n","Для REINFORCE вам нужна модель, которая берёт на вход состояние (каким-то образом закодированное) и возвращает вероятностное распределение действий в нём.\n","\n","Старайтесь не перемудрить — в общем случае сети для RL могут быть [довольно сложными](https://d4mucfpksywv.cloudfront.net/research-covers/openai-five/network-architecture.pdf), но CartPole не стоит того, чтобы писать глубокие архитектуры."]},{"cell_type":"code","metadata":{"id":"oynCK8dd27Ht","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f48sXTXa27Hv","colab_type":"code","colab":{}},"source":["agent = nn.Sequential(\n","    nn.Linear(4, 8),\n","    nn.ReLU(),\n","    nn.Linear(8, 2),\n","    nn.LogSoftmax(dim=1)  # важно, что на выходе должны быть не вероятности, а логиты\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ggyG8ils27Hx","colab_type":"text"},"source":["OpenAI Gym работает с numpy, а не напрямую с фреймворками. Для удобства, напишите функцию-обёртку, которая принимает состояния (`numpy array` размера `[batch, state_shape]`) и возвращает вероятности (размера `[batch, n_actions]]`, должны суммироваться в единицу)."]},{"cell_type":"code","metadata":{"id":"JXJ3H3LS27Hy","colab_type":"code","colab":{}},"source":["def predict_proba(states):\n","    # сконвертируйте состояния в тензор\n","    # вычислите логиты\n","    # вызовите софтмакс, чтобы получить веряотности\n","    return # ..."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DZnYJWRK27H0","colab_type":"code","colab":{}},"source":["test_states = np.array([env.reset() for _ in range(5)])\n","test_probas = predict_proba(test_states)\n","assert isinstance(test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n","assert tuple(test_probas.shape) == (test_states.shape[0], n_actions), \"wrong output shape: %s\" % np.shape(test_probas)\n","assert np.allclose(np.sum(test_probas, axis = 1), 1), \"probabilities do not sum to 1\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fRoxP08z27H1","colab_type":"text"},"source":["## Тестовый прогон\n","\n","Хоть наша модель не обучена, её уже можно использовать, чтобы играть в произвольной среде."]},{"cell_type":"code","metadata":{"id":"hoM9YW5H27H2","colab_type":"code","colab":{}},"source":["def generate_session(t_max=1000):\n","    \"\"\" \n","    Играет одну сессию REINFORCE-агентом и возвращает последовательность состояний,\n","    действий и наград, которые потом будут использоваться при обучении.\n","    \"\"\"\n","    \n","    # тут будем хранить сессию\n","    states, actions, rewards = [],[],[]\n","    \n","    s = env.reset()\n","    \n","    for t in range(t_max):\n","        \n","        # вероятности следующих действий, aka p(a|s)\n","        action_probas = predict_proba(np.array([s]))[0] \n","        \n","        # сэмплируйте оттуда действие (посказка: np.random.choice)\n","        a = # ...\n","        \n","        new_s, r, done, info = env.step(a)\n","        \n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","        \n","        s = new_s\n","        if done:\n","            break\n","            \n","    return states, actions, rewards"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RChXl6GD27H3","colab_type":"code","colab":{}},"source":["# протестируйте\n","states, actions, rewards = generate_session()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d0xhuQgh27H5","colab_type":"text"},"source":["### Computing cumulative rewards"]},{"cell_type":"code","metadata":{"id":"DOF_VZIv27H5","colab_type":"code","colab":{}},"source":["def get_cumulative_rewards(rewards, gamma=0.99):\n","    \"\"\"\n","    Принимает массив ревардов и возвращает discounted массив по следующей формуле:\n","    \n","        G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","    \n","    Тут нет ничего сложного -- итерируйтесь от последнего до первого\n","    и насчитывайте G_t = r_t + gamma*G_{t+1} рекуррентно.\n","    \"\"\"\n","    \n","    # ...\n","    return # ..."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4H6MNAo27H7","colab_type":"code","colab":{}},"source":["get_cumulative_rewards(rewards)\n","assert len(get_cumulative_rewards(list(range(100)))) == 100\n","assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n","                   [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n","                   [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n","                   [0, 0, 1, 2, 3, 4, 0])\n","print(\"Вроде норм\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"24NHQ2_n27H8","colab_type":"text"},"source":["#### Loss function and updates\n","\n","Вспомним, что нам нужно оптимизировать\n","\n","$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n","\n","\n","Используя REINFORCE, нам в алгоритме по сути нужно максимизировать немного другую функцию:\n","\n","$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n","\n","Когда мы будем вычислять её градиент, мы получим в точности policy gradient из REINFORCE."]},{"cell_type":"code","metadata":{"id":"63Vhul4D27H8","colab_type":"code","colab":{}},"source":["def to_one_hot(y_tensor, n_dims=None):\n","    \"\"\" Конвертирует целочисленный вектор в one-hot матрицу. \"\"\"\n","    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n","    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n","    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n","    return y_one_hot"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BsBAlZID27H9","colab_type":"code","colab":{}},"source":["# тут определите оптимизатор для модели\n","# например, Adam с дефолтными параметрами\n","\n","def train_on_session(states, actions, rewards, gamma = 0.99):\n","    \n","    states = torch.tensor(states, dtype=torch.float32)\n","    actions = torch.tensor(actions, dtype=torch.int32)\n","    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n","    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n","    \n","    logprobas = # ...\n","    probas = # ...\n","    \n","    assert all(isinstance(v, torch.Tensor) for v in [probas, logprobas]), \\\n","        \"please use compute using torch tensors and don't use predict_proba function\"\n","    \n","    # выберем и просуммируем лог-вероятности только для выбранных действий\n","    logprobas_for_actions = torch.sum(logprobas * to_one_hot(actions), dim=1)\n","    \n","    J_hat = # формула для REINFORCE\n","    \n","    # опционально: энтропийная регуляризация\n","    entropy_reg = # вычислите среднюю энтропию вероятностей; не забудьте знак!\n","    \n","    loss = - J_hat - 0.1 * entropy_reg\n","    \n","    # шагните в сторону градиента\n","    # ....\n","    \n","    # верните ревард сессии, чтобы потом их печатать\n","    return np.sum(rewards)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"spOFhZuG27H-","colab_type":"text"},"source":["## Само обучение"]},{"cell_type":"code","metadata":{"id":"tp6P9a6-27H-","colab_type":"code","colab":{}},"source":["for i in range(100):\n","    \n","    rewards = [train_on_session(*generate_session()) for _ in range(100)]\n","    \n","    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n","\n","    if np.mean(rewards) > 500:\n","        print (\"Победа!\")\n","        break"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLe3kyRO27IA","colab_type":"text"},"source":["## Видосик"]},{"cell_type":"code","metadata":{"id":"XSw58JzT27IA","colab_type":"code","colab":{}},"source":["import gym.wrappers\n","env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n","sessions = [generate_session() for _ in range(100)]\n","env.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e9pXPEqM27IB","colab_type":"code","colab":{}},"source":["from IPython.display import HTML\n","import os\n","\n","video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(\"./videos/\"+video_names[-1]))"],"execution_count":0,"outputs":[]}]}
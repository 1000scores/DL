{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"name":"RNN.ipynb","provenance":[],"collapsed_sections":["hGdYtkjTkHQq"]}},"cells":[{"source":["https://www.youtube.com/watch?v=8M7DHl-aae8"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"HXd4o-HZkHQN","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import os\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pRIVQmrukHQQ","colab_type":"text"},"source":["## Ссылки:\n","* [Chris Olah's blog (LSTM/GRU)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n","* [PyTorch tutorial - RNN for name classification](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)\n","* [MNIST classification with RNN tutorial](https://medium.com/dair-ai/building-rnns-is-fun-with-pytorch-and-google-colab-3903ea9a3a79)\n","* [Good tutorials about Torch sentiment](https://github.com/bentrevett/pytorch-sentiment-analysis)"]},{"cell_type":"markdown","metadata":{"id":"NltJUttHkHQR","colab_type":"text"},"source":["## Vanilla RNN"]},{"cell_type":"markdown","metadata":{"id":"K-4oBp1MkHQS","colab_type":"text"},"source":["<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\" width=\"600\">"]},{"cell_type":"markdown","metadata":{"id":"PiAC7p6JkHQT","colab_type":"text"},"source":["$$\\Large h_{i+1} = tanh(W_x \\cdot X_{i+1} + W_y \\cdot h_{i})$$"]},{"cell_type":"markdown","metadata":{"id":"U9hhj5mLkHQT","colab_type":"text"},"source":["Рекурретные нейросети нужны для работы с **последовательными данными** произвольной длины. Они представляют собой абстрактные ячейки, у которых есть какая-то **память** (hidden state), которая обновляется после обработки очередной порции данных.\n","\n","Если в самом простом виде, то в рекуррентных сетках для одного входного вектора $x_{(t)}$ и одного слоя рекуррентной сети справедливо такое соотношение:\n","\n","$$y_{(t)} = \\phi (x_{(t)}^T \\cdot w_x + y_{(t-1)}^T \\cdot w_y + b)$$\n","\n","где \n","* $x(t)$ — входной вектор на текущем шаге;\n","* $y(t)$ — выходной вектор на текущем шаге;\n","* $w_x$ — вектор весов нейронов для входа;\n","* $w_y$ — вектор весов нейронов для выхода;\n","* $y(t-1)$ — выходной вектор с прошлого шага (для первого шага этот вектор нулевой);\n","* $b$ — bias;\n","* $\\phi$ — какая-то функция активации (например, ReLU).\n","\n","Эту ячейку применяют по очереди ко всей последовательности, пробрасывая hidden state с предыдущего состояния. С точки зрения построения вычислительного графа это выглядит так:\n","\n","<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" width=\"600\">\n","\n","То есть если зафиксировать длину последовательности, то мы получим обычный фиксированный ациклический граф вычислений, в котором просто пошерены параметры всех ячеек.\n","\n","### Упрощение формулы\n","\n","Снова немножко математики чтобы привести формулу выше к более удобному виду.\n","\n","Представим, что на вход подается не один вектор $x_{(t)}$, а целый мини-батч размера $m$ таких векторов $X_{(t)}$, соответственно все дальнейшие размышления мы уже производим в матричном виде:\n","\n","$$ Y_{(t)} = \\phi(X_{(t)}^T \\cdot W_x + Y_{(t-1)}^T \\cdot W_y + b) = \\phi([X_{(t)} Y_{(t-1)}] \\cdot W + b) $$\n","где\n","$$ W = [W_x W_y]^T $$\n","\n","*Операция в квадратных скобках — конкатенация матриц\n","\n","По размерностям:\n","* $Y_{(t)}$ — матрица [$m$ x n_neurons]\n","* $X_{(t)}$ — матрица [$m$ x n_features]\n","* $b$ — вектор длины n_neurons\n","* $W_x$ — веса между входами и нейронами размерностью [n_features x n_neurons]\n","* $W_y$ — веса связей с прошлым выходом размерностью [n_neurons x n_neurons]"]},{"cell_type":"markdown","metadata":{"id":"Gq6dnwWWkHQU","colab_type":"text"},"source":["# RNN from scratch\n","\n","**Disclaimer:** не используйте самописные RNN-ки в реальной жизни.\n","\n","Давайте реализуем торчовый модуль, который это реализует."]},{"cell_type":"code","metadata":{"id":"dSkcPzlokHQW","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super().__init__()\n","\n","        self.hidden_size = hidden_size\n","        # <создать Wx, Wy?>\n","\n","    def forward(self, input_data, hidden):\n","        # <использовать Wx, Wy для полученния нового hidden>\n","        return hidden\n","\n","    def init_hidden(self):\n","        return torch.zeros(1, self.hidden_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w9n3gNkPkHQY","colab_type":"code","colab":{}},"source":["input_feature_size = 6\n","hidden_size=5\n","batch_size=1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2rqMgLsqkHQd","colab_type":"code","colab":{}},"source":["rnn = RNN(input_size=input_feature_size, hidden_size=hidden_size)\n","initial_hidden = rnn.init_hidden(batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZssjzwGDkHQg","colab_type":"code","colab":{}},"source":["input_example = torch.rand([batch_size, input_feature_size])\n","new_hidden = rnn(input_example, initial_hidden)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QdiE8vn-kHQi","colab_type":"code","colab":{}},"source":["print(new_hidden.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Io1NEsIKkHQm","colab_type":"code","colab":{}},"source":["print(\"initial_hidden: \", initial_hidden.numpy())\n","print(\"new_hidden: \", new_hidden.detach().numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hIwKBaQWkHQo","colab_type":"code","colab":{}},"source":["new_hidden = rnn(input_example, new_hidden)\n","print(\"new_hidden: \", new_hidden.detach().numpy())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hGdYtkjTkHQq","colab_type":"text"},"source":["**Задание**. Модифицируйте код так, чтобы на вход можно было подавать батчи размером больше 1."]},{"cell_type":"markdown","metadata":{"id":"aGi7O7qekHQr","colab_type":"text"},"source":["# Классификация картинок с RNN\n","\n","Представьте, что у вас есть какая-то длинная картинка, в которой свёртки точно не зайдут. Например, снимки со спутника, спектрограмма или длиннокот."]},{"cell_type":"markdown","metadata":{"id":"v0H9o53CkHQs","colab_type":"text"},"source":["Можно обработать их построчно с помощью рекуррентных сетей — просто подавать в качестве входа все пиксели очередной строки.\n","\n","<img src=\"https://cdn-images-1.medium.com/max/2000/1*wFYZpxTTiXVqncOLQd_CIQ.jpeg\" width=\"800\">"]},{"cell_type":"code","metadata":{"id":"xaQ0QhgfkHQt","colab_type":"code","colab":{}},"source":["!mkdir data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kmOhDZDEkHQv","colab_type":"text"},"source":["Загружаем данные"]},{"cell_type":"code","metadata":{"id":"_ahhqfzgkHQw","colab_type":"code","colab":{}},"source":["import torchvision\n","import torchvision.transforms as transforms\n","\n","BATCH_SIZE = 64\n","\n","# переводим все в тензоры\n","transform = transforms.Compose(\n","    [transforms.ToTensor()])\n","\n","trainset = torchvision.datasets.MNIST(root='./data', train=True,\n","                                      download=True, transform=transform)\n","\n","testset = torchvision.datasets.MNIST(root='./data', train=False,\n","                                     download=True, transform=transform)\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n","                                          shuffle=True, num_workers=2)\n","\n","testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n","                                         shuffle=False, num_workers=2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zb3byVwpkHQz","colab_type":"code","colab":{}},"source":["!ls -lh data/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wVCuLHrmkHQ4","colab_type":"code","colab":{}},"source":["%pylab inline\n","import numpy as np\n","\n","def imshow(img):\n","    #img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n","# get some random training images\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NSBQfZBrkHQ8","colab_type":"text"},"source":["## Как выглядит классификация с RNN в общем виде "]},{"cell_type":"markdown","metadata":{"id":"xTIzNZ09kHQ9","colab_type":"text"},"source":["<img src=\"https://cdn-images-1.medium.com/max/1600/1*vhAfRLlaeOXZ-bruv7Ostg.png\" width=\"400\">"]},{"cell_type":"code","metadata":{"id":"uZ2UZw8vkHQ-","colab_type":"code","colab":{}},"source":["class ImageRNN(nn.Module):\n","    def __init__(self, batch_size, n_steps, n_inputs, n_neurons, n_outputs):\n","        super().__init__()\n","        \n","        self.n_neurons = n_neurons\n","        self.batch_size = batch_size\n","        self.n_steps = n_steps\n","        self.n_inputs = n_inputs\n","        self.n_outputs = n_outputs\n","        \n","        self.basic_rnn = nn.RNN(self.n_inputs, self.n_neurons) \n","        \n","        self.FC = nn.Linear(self.n_neurons, self.n_outputs)\n","        \n","    def init_hidden(self,):\n","        # (num_layers, batch_size, n_neurons)\n","        return (torch.zeros(1, self.batch_size, self.n_neurons))\n","        \n","    def forward(self, X):\n","        # transforms X to dimensions: n_steps X batch_size X n_inputs\n","        X = X.permute(1, 0, 2) \n","        \n","        self.batch_size = X.size(1)\n","        self.hidden = self.init_hidden()\n","        \n","        lstm_out, self.hidden = self.basic_rnn(X, self.hidden)      \n","        out = self.FC(self.hidden)\n","        \n","        return out.view(-1, self.n_outputs) # batch_size X n_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8FZWcJTokHRB","colab_type":"code","colab":{}},"source":["N_STEPS = 28\n","N_INPUTS = 28\n","N_NEURONS = 150\n","N_OUTPUTS = 10\n","N_EPHOCS = 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLaGmqMnkHRE","colab_type":"code","colab":{}},"source":["dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","model = ImageRNN(BATCH_SIZE, N_STEPS, N_INPUTS, N_NEURONS, N_OUTPUTS)\n","logits = model(images.view(-1, 28,28))\n","print(logits[0:10])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"snsRIyjtkHRJ","colab_type":"text"},"source":["## Обучаем"]},{"cell_type":"code","metadata":{"id":"bDAvHQlTkHRK","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","# Device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Model instance\n","model = ImageRNN(BATCH_SIZE, N_STEPS, N_INPUTS, N_NEURONS, N_OUTPUTS)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","def get_accuracy(logit, target, batch_size):\n","    ''' Obtain accuracy for training round '''\n","    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n","    accuracy = 100.0 * corrects/batch_size\n","    return accuracy.item()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Flff1olHkHRN","colab_type":"code","colab":{}},"source":["for epoch in range(N_EPHOCS):\n","    train_running_loss = 0.0\n","    train_acc = 0.0\n","    model.train()\n","    \n","    # TRAINING ROUND\n","    for i, data in enumerate(trainloader):\n","         # zero the parameter gradients\n","        optimizer.zero_grad()\n","        \n","        # reset hidden states\n","        model.hidden = model.init_hidden() \n","        \n","        # get the inputs\n","        inputs, labels = data\n","        inputs = inputs.view(-1, 28,28) \n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_running_loss += loss.detach().item()\n","        train_acc += get_accuracy(outputs, labels, BATCH_SIZE)\n","         \n","    model.eval()\n","    print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' \n","          %(epoch, train_running_loss / i, train_acc/i))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6TPiwYZgkHRS","colab_type":"text"},"source":["### Смотрим что на тесте"]},{"cell_type":"code","metadata":{"id":"6j98n0YbkHRT","colab_type":"code","colab":{}},"source":["test_acc = 0.0\n","for i, data in enumerate(testloader, 0):\n","    inputs, labels = data\n","    inputs = inputs.view(-1, 28, 28)\n","\n","    outputs = model(inputs)\n","\n","    test_acc += get_accuracy(outputs, labels, BATCH_SIZE)\n","        \n","print('Test Accuracy: %.2f'%( test_acc/i))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kM_LmfEWkHRV","colab_type":"text"},"source":["# Сентимент анализ\n","\n","Домашка — классифицировать отзывы с IMDB на положительный / отрицательный только по тексту.\n","\n","<img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/raw/bf8cc46e4823ebf9af721b595501ad6231c73632/assets/sentiment1.png\">\n","\n","Суть такая же, только нужно предобработать тексты — каждому слову сопоставить обучаемый вектор (embedding), который пойдёт дальше в RNN."]},{"cell_type":"code","metadata":{"id":"-jprChrLiWAh","colab_type":"code","colab":{}},"source":["# это уберет боль работы с текстами\n","!pip install torchtext\n","!python -m spacy download en"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sQ0jIrMtkHRW","colab_type":"text"},"source":["**Примечание.** Torchtext уже не очень живой проект, а в spacy нет русского.\n"]},{"cell_type":"code","metadata":{"id":"vpiI5vjUkHRW","colab_type":"code","colab":{}},"source":["import torch\n","from torchtext import data\n","\n","SEED = 1234\n","\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","TEXT = data.Field(tokenize='spacy')\n","LABEL = data.LabelField(dtype=torch.float)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOCZ7pthkHRY","colab_type":"code","colab":{}},"source":["from torchtext import datasets\n","train_data, test_data = datasets.IMDB.splits(TEXT, LABEL, root=\"./data\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KPHUYfw2kHRb","colab_type":"code","colab":{}},"source":["ls -lh data/imdb/aclImdb/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3Ow6Q3okHRf","colab_type":"code","colab":{}},"source":["print(f'Number of training examples: {len(train_data)}')\n","print(f'Number of testing examples: {len(test_data)}')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQquyqCEkHRk","colab_type":"code","colab":{}},"source":["print(vars(train_data.examples[0]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BufoUgSCkHRn","colab_type":"code","colab":{}},"source":["# Сделаем еще eval\n","import random\n","\n","train_data, valid_data = train_data.split(random_state=random.seed(SEED))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7amSe4ikHRp","colab_type":"code","colab":{}},"source":["# Сделаем словарь\n","TEXT.build_vocab(train_data, max_size=25000)\n","LABEL.build_vocab(train_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IWw3xa1gkHRr","colab_type":"code","colab":{}},"source":["print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n","print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z-_ejTnFkHRx","colab_type":"code","colab":{}},"source":["vars(LABEL.vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PhJ0nG7akHRz","colab_type":"text"},"source":["Почему 25002, а не 25000?\n","Потому что $<unk>$ и $<pad>$\n","\n","<img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/raw/bf8cc46e4823ebf9af721b595501ad6231c73632/assets/sentiment6.png\" width=\"160\">"]},{"cell_type":"code","metadata":{"id":"qxMRpe-hkHRz","colab_type":"code","colab":{}},"source":["print(TEXT.vocab.freqs.most_common(20))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1kPcwN6qkHR1","colab_type":"text"},"source":["* stoi (string to int)\n","* itos (int to string)"]},{"cell_type":"code","metadata":{"id":"fVf_llCokHR2","colab_type":"code","colab":{}},"source":["print(TEXT.vocab.itos[:10])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K430yfrfkHR3","colab_type":"code","colab":{}},"source":["print(LABEL.vocab.stoi)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qzyXSNLTkHR5","colab_type":"code","colab":{}},"source":["BATCH_SIZE = 64\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# собираем батчи так, чтобы в каждом батче были примеры наиболее похожей длины\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size=BATCH_SIZE,\n","    device=device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z1NZqp-6kHR9","colab_type":"text"},"source":["## Делаем модель"]},{"cell_type":"markdown","metadata":{"id":"cgP2m_x0kHR-","colab_type":"text"},"source":["<img src=\"https://github.com/bentrevett/pytorch-sentiment-analysis/raw/bf8cc46e4823ebf9af721b595501ad6231c73632/assets/sentiment7.png\" width=\"450\">"]},{"cell_type":"markdown","metadata":{"id":"r9OlXUAvkHR_","colab_type":"text"},"source":["* В эмбеддер (emb = [torch.nn.Embedding(num_embeddings, embedding_dim)](https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding)) запихиваем тензор размерностью **[sentence length, batch size]**\n","* Эмбеддер возвращает тензор размерностью **[sentence length, batch size, embedding dim]**\n","* RNN (torch.nn.RNN(embedding_dim, hidden_dim)) возвращает 2 тензора, *output* размера [sentence length, batch size, hidden dim] и *hidden* размера [1, batch size, hidden dim]"]},{"cell_type":"code","metadata":{"id":"HrizbizOkHR_","colab_type":"code","colab":{}},"source":["class RNN(nn.Module):\n","    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(input_dim, embedding_dim)\n","        self.rnn = ?\n","        self.fc = ? # можно добавить линейный слой, который делает проекцию в 2 класса\n","    \n","\n","        #text,shape = [sent len, batch size]\n","        \n","        embedded = ?\n","        \n","        #embedded.shape = [sent len, batch size, emb dim]\n","        \n","        output, hidden = ?\n","        \n","        #output.shape = [sent len, batch size, hid dim]\n","        #hidden.shape = [1, batch size, hid dim]\n","        \n","#         assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n","        \n","        return self.fc(hidden.squeeze(0))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ctEOkK-6kHSB","colab_type":"code","colab":{}},"source":["# код обучения тоже пишите вы:)"],"execution_count":0,"outputs":[]}]}
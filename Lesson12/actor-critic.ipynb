{
 "cells": [
  {
   "source": [
    "https://www.youtube.com/watch?v=d2DiiIvt_Qk\n",
    "https://www.youtube.com/watch?v=hSZDrDn5txM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#If you are running on a server, launch xvfb to record game videos\n",
    "#Please make sure you have xvfb installed\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С теорией могут помочь эти слайды: http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf.pdf\n",
    "\n",
    "А могут и не помочь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мастерами кунг-фу не рождаются\n",
    "\n",
    "Сегодня мы будем играть в Atari [KungFuMaster](https://gym.openai.com/envs/KungFuMaster-v0/), на этот раз используя рекуррентные сетки.\n",
    "\n",
    "![kung-fu-master](https://lh3.googleusercontent.com/fzmeaDZPcTJqlrdA_NMhXOFkafTiM5JnBxUkYdgH_FlAjoCVWYmGbxia16MwnIpu1g=w412-h220-rw)\n",
    "\n",
    "Эта игра уже намного сложнее удержания палки в вертикальном положении. Состояние — это RGB картинка монитора (трёхмерный массив размера 210x160x3), а на выбор у вас есть целых 14 действий — разные перемещения, удары, прыжки и прочее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# убедитесь, что atari_util.py находится в той же директории,\n",
    "# что и эта тетрадка, а также поставьте atari-py:\n",
    "!pip install atari-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    # размерность картинки слишком большая: \n",
    "    # давайте обрежем её и перегоним в чб:\n",
    "    env = PreprocessAtari(env, height=42, width=42,\n",
    "                          crop = lambda img: img[60:-30, 15:],\n",
    "                          color=False, n_frames=1)\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Исходное изображение')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Что видит агент')\n",
    "plt.imshow(s.reshape([42,42]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Частично-наблюдаемые MDP\n",
    "\n",
    "Наша игра — это на самом деле POMDP: агент знает тайминги, когда враги спавнятся и двигаются, но понять это по одной картинке он не может — ему нужна какая-то память для этого.\n",
    "\n",
    "Нам нужна какая-то сеть, которая использует память рекуррентной сети. Например, такая:\n",
    "\n",
    "<img src='https://github.com/yandexdataschool/Practical_RL/blob/spring19/week08_pomdp/img1.jpg?raw=true' width='500px'> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# [batch, channel, w, h] -> [batch, units]\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions, reuse=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(1, 32, kernel_size=(3,3), stride=(2,2))\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=(2,2))\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.hid = nn.Linear(512, 128)\n",
    "        self.rnn = nn.LSTMCell(128, 128)\n",
    "\n",
    "        self.logits = nn.Linear(128, n_actions)\n",
    "        self.state_value = nn.Linear(128, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, prev_state, obs_t):\n",
    "        \"\"\"\n",
    "        Принимает предыдущее состояние (память) и наблюдени,\n",
    "        возвращает следующее состояние и пару из политики (actor) и оценки состояния (critic) \n",
    "        \"\"\"\n",
    "        \n",
    "        # ...\n",
    "        \n",
    "        new_state = # ...\n",
    "        logits = # ...\n",
    "        state_value = # ...\n",
    "        \n",
    "        return new_state, (logits, state_value)\n",
    "    \n",
    "    def get_initial_state(self, batch_size):\n",
    "        \"\"\"Возвращает память агента в начале игры.\"\"\"\n",
    "        return torch.zeros((batch_size, 128)), torch.zeros((batch_size, 128))\n",
    "    \n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"Делает случайное действие, в соответствие с предсказанными вероятностями.\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        return torch.multinomial(probs, 1)[:, 0].data.numpy()\n",
    "    \n",
    "    def step(self, prev_state, obs_t):\n",
    "        \"\"\"Подобно forward, но obs_t это не торчевый тензор.\"\"\"\n",
    "        obs_t = torch.tensor(np.array(obs_t), dtype=torch.float32)\n",
    "        (h, c), (l, s) = self.forward(prev_state, obs_t)\n",
    "        return (h.detach(), c.detach()), (l.detach(), s.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel_games = 5\n",
    "gamma = 0.99\n",
    "\n",
    "agent = SimpleRecurrentAgent(obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [env.reset()]\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), state)\n",
    "print(\"Action logits:\\n\", logits)\n",
    "print(\"State values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "\n",
    "Напишем функцию, которая меряет средний reward агента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Играет игру от начала до конца и возвращает награды на каждом шаге.\"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        observation = env.reset()\n",
    "        prev_memories = agent.get_initial_state(1)\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            new_memories, readouts = agent.step(prev_memories, observation[None, ...])\n",
    "            action = agent.sample_actions(readouts)\n",
    "\n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "            prev_memories = new_memories\n",
    "            if done: break\n",
    "                \n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "env_monitor.close()\n",
    "print(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# видосик\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параллельные игры\n",
    "\n",
    "Введем EnvPool — это такая абстракция для управления множественными средами:\n",
    "\n",
    "![](https://github.com/yandexdataschool/Practical_RL/blob/spring19/week08_pomdp/img2.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_pool import EnvPool\n",
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/yandexdataschool/Practical_RL/blob/spring19/week08_pomdp/img3.jpg?raw=true)\n",
    "\n",
    "*Роллаут* это просто последовательность наблюдений, действий и наград, которые произошли последовательно друг за другом. Обратите внимание, что это не то же самое, что сессия:\n",
    "\n",
    "* Первое состояние не обязательно начальное для среды.\n",
    "* Последнее состояние не обязательно терминальное.\n",
    "* По соображениям эффективности, мы будем сэмплировать несколько параллельных роллаутов одновременно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделать 10 шагов в каждой из n_parallel_games игр\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)\n",
    "print(\"Mask shape:\", rollout_mask.shape)\n",
    "print(\"Observations shape: \",rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Виды RL-я\n",
    "\n",
    "Value-based:\n",
    "* Оценивают value состояния — какая там ожидаемая награда\n",
    "* Политики нет, точнее она неявная — идём туда, где больше value. При обучении пользуемся $\\epsilon$-greedy.\n",
    "\n",
    "Policy-based:\n",
    "* Оценивают оптимальную policy.\n",
    "* Value function нет.\n",
    "* (Мы делали ровно это в прошлый раз.)\n",
    "\n",
    "Actor-critic:\n",
    "* Оцениваем и policy, и value (независимо обучаем две сетки).\n",
    "* Это позволяет сильно улучшить сходимость policy, потому что с помощью value-функции можно нормировать реворды из состояния, что уменьшит дисперсию градиентов.\n",
    "\n",
    "![actor-critic](https://cs.wmich.edu/~trenary/files/cs5300/RLBook/figtmp34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция потерь\n",
    "\n",
    "Наш лосс будет состоять из трёх компонент:\n",
    "\n",
    "* **Policy loss:**\n",
    " $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
    "  * Напоминание: сама по себе эта функция не имеет смысла, и была выведена так, что\n",
    "  * $ \\nabla \\hat J = {1 \\over N} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
    "  * То есть если мы максимизируем (не перепутайте тут знак) $\\hat J$ градиентным спуском, то мы максимизируем ожидаемый ревард.\n",
    "\n",
    "\n",
    "* **Value loss:**\n",
    "  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
    "  * Если мы будем минимизировать его, то $V(s)$ сойдётся к $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
    "\n",
    "\n",
    "* **Энтропийная регуляризация** — вы это уже видели:\n",
    "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
    "  * Если максимизировать энтропию, то агент не захочет предсказывать нулевые вероятности для действий (a.k.a. exploration)\n",
    "  \n",
    "  \n",
    "Так что мы будем оптимизировать какую-то линейную комбинацию $L_{td}$, $- \\hat J$ и $-H$\n",
    "\n",
    "**Кстати:** раз мы при обучении рассматриваем роллауты на $T$ последовательных шагов, то мы можем бесплатно использовать более точные формулы для $A(s_t, a_t)$:\n",
    "  * Последний шаг: $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n",
    "  * Предпоследний шаг: $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n",
    "  * ...И так далее. Так агент обучается намного быстрее, потому что он меньше зависит от шумной оценки $V$ и больше на сами награды.\n",
    "  * По этой причине хорошей идеей будет увеличить rollout_len (до >=20). Ещё его лучше увеличивать с течением времени, но не обязательно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    y_tensor = torch.tensor(y, dtype=torch.int64).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-5)\n",
    "\n",
    "def train_on_rollout(states, actions, rewards, is_not_done, prev_memory_states, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Берет роллаут -- последовательность состояний, действий и наград, полученных из generate_session.\n",
    "    Обновляет веса агента через policy gradient.\n",
    "    Менять параметры Adam-а не рекомендуется.\n",
    "    \"\"\"\n",
    "    \n",
    "    # сконвертируем всё в torch.tensor\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32)   # [batch_size, time, c, h, w]\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.int64)   # [batch_size, time]\n",
    "    rewards = torch.tensor(np.array(rewards), dtype=torch.float32) # [batch_size, time]\n",
    "    is_not_done = torch.tensor(is_not_done.astype('float32'), dtype=torch.float32)  # [batch_size, time]\n",
    "    rollout_length = rewards.shape[1] - 1\n",
    "\n",
    "    # теперь нужно посчитать логиты, вероятности и лог-вероятности\n",
    "    # больше для лосса нам ничего не нужно от модели\n",
    "    \n",
    "    memory = [m.detach() for m in prev_memory_states]\n",
    "    \n",
    "    logits = []\n",
    "    state_values = []\n",
    "    for t in range(rewards.shape[1]):\n",
    "        obs_t = states[:, t]\n",
    "        \n",
    "        # вычислите моделью logits_t и values_t.\n",
    "        # и зааппендьте их к спискам logits и state_values\n",
    "        \n",
    "        memory, (logits_t, values_t) = <YOUR CODE>\n",
    "        \n",
    "        logits.append(logits_t)\n",
    "        state_values.append(values_t)\n",
    "        \n",
    "    logits = torch.stack(logits, dim=1)\n",
    "    state_values = torch.stack(state_values, dim=1)\n",
    "    probas = torch.softmax(logits, dim=2)\n",
    "    logprobas = torch.log_softmax(logits, dim=2)\n",
    "        \n",
    "    # выбираем лог-вероятности для реальных действий -- log pi(a_i|s_i)\n",
    "    actions_one_hot = to_one_hot(actions, n_actions).view(\n",
    "        actions.shape[0], actions.shape[1], n_actions)\n",
    "    logprobas_for_actions = torch.sum(logprobas * actions_one_hot, dim = -1)\n",
    "    \n",
    "    # Теперь посчитайте две основные компоненты лосса:\n",
    "    # 1) Policy gradient\n",
    "    # Примечание: не забываейте делать .detach() для advantage.\n",
    "    # Ещё лучше использовать mean, а не sum, чтобы lr не масштабировать.\n",
    "    # Можно исползовать тут циклы, если хотите.\n",
    "    J_hat = 0  # посчитаем ниже\n",
    "    \n",
    "    # 2) Temporal difference MSE\n",
    "    value_loss = 0  # посчитаем ниже\n",
    "    \n",
    "    cumulative_returns = state_values[:, -1].detach()\n",
    "    for t in reversed(range(rollout_length)):\n",
    "        r_t = rewards[:, t]                        # текущие reward-ы\n",
    "        V_t = state_values[:, t]                   # value текущих состоияний\n",
    "        V_next = state_values[:, t + 1].detach()   # value следующих состояний\n",
    "        logpi_a_s_t = logprobas_for_actions[:, t]  # вероятности сделать нужное действие\n",
    "        \n",
    "        # G_t = r_t + gamma * G_{t+1}, как в прошлый раз на reinforce\n",
    "        cumulative_returns = G_t = r_t + gamma * cumulative_returns\n",
    "        \n",
    "        # Посчитайте MSE для V(s)\n",
    "        value_loss += # ...\n",
    "        \n",
    "        # посчитайте advantage A(s_t, a_t), используя cumulative returns и V(s_t) в качестве бейзлайна\n",
    "        advantage = # ...\n",
    "        advantage = advantage.detach()\n",
    "        \n",
    "        # посчитаем весь policy loss (-J_hat).\n",
    "        J_hat += <YOUR CODE>\n",
    "    \n",
    "    entropy_reg = # compute entropy regularizer\n",
    "    \n",
    "    # усредним всё это дело с какими-то весами\n",
    "    loss = -J_hat / rollout_length +\\\n",
    "           value_loss / rollout_length +\\\n",
    "           -0.01 * entropy_reg\n",
    "    \n",
    "    # сделайте шаг против градиента\n",
    "    # ...\n",
    "    \n",
    "    return loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим работу\n",
    "memory = list(pool.prev_memory_states)\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "\n",
    "train_on_rollout(rollout_obs, rollout_actions, rollout_rewards, rollout_mask, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "\n",
    "rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(15000):  \n",
    "    \n",
    "    memory = list(pool.prev_memory_states)\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "    train_on_rollout(rollout_obs, rollout_actions, rollout_rewards, rollout_mask, memory)    \n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n",
    "        clear_output(True)\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(moving_average(np.array(rewards_history),span=10), label='rewards ewma@10')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if rewards_history[-1] >= 10000:\n",
    "            print(\"Yellow belt awarded\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Агент сейчас заперт в бесконечном цикле жестокости и насилий.\n",
    "\n",
    "### Дебаг\n",
    "\n",
    "Награды за сессию должны в целом идти в верх, но это нормально, если они будут колебаться (очень сильно). Так же нормально, что агент ничего солидного не выучит после 10к первых итераций. Что-то не так идет только тогда, реворд нулевой и не поднимается спустя 2-3 эвалов подряд.\n",
    "\n",
    "Мы используем policy-based метод, и тут полезно смотреть на энтропию политики (та штука, которую мы использовали как регуляризацию). Если она становится слишком мала ($< 0.1$) до того момента, когда ваш агент получит желтый пояс, то что-то идет не так.\n",
    "\n",
    "Если что-то не так, то проверьте в первую очередь следующее:\n",
    "* Какой-то баг в энтропии: $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Сеть слишком быстро сходится к чему-то неоптимальному. Увеличьте коэффициент регуляризации.\n",
    "* Взрывающиеся градиенты — обрежьте градиенты (`gradient_clip`) и, возможно, используйте нейронку поменьше.\n",
    "\n",
    "При дебаге полезно запустить `logits, values = agent.step(batch_states)` и глазами посмотреть на логиты и value: возможно, там будут какие-нибудь NaN-ы, безумно большие числа или нули. Отловите этот момент как только это случилось и попытайтесь понять, что пошло не так."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Видосик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20,)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # убедитесь, что файл правильный"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}